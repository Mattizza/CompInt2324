{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from SimBoard import Board\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Lab's objective is to develop an agent capable of playing Tic-Tac-Toe using Reinforcement Learning (RL). Through the application of simple strategies and techniques, I have managed to create an agent that, after approximately 6 minutes of training (equivalent to 500.000 random games + 300.000 policy games), achieves a 73% win rate and avoids losses in 85% of cases. The results are quite satisfactory, given the agent's straightforward design and the relatively short training time. It is worth noting that I deliberately restricted the agent to always making the second move, providing an advantage to the opponent, and thus establishing a lower bound on the performances.\n",
    "\n",
    "---\n",
    "\n",
    "# Methodology\n",
    "\n",
    "To ease the evaluation process and reduce the number of potential configurations, I made use of symmetries and rotations when assessing states. Players make their moves on a board, which are then translated to a canonical form serving as a reference to which all other configurations can be mapped. Significantly, evaluations are exclusively conducted on the canonical state, enhancing the efficiency of state evaluation. This approach has demonstrated an increase in the win rate by several percentage points, proving to be a valuable technique for a smarter states exploration. Moreover, the evaluation of states, initially performed during a training phase using only random games, has been extended to games played by an agent adopting a policy. While the first 500.000 evaluations were conducted randomly, an additional 200.000 evaluations were performed using both a random agent and an agent employing a policy. This fine-tuning has significantly increased the win rate by some percentage points. In terms of move selection, the agent adopts a greedy strategy, prioritizing moves that maximize immediate rewards in the next configuration. While deeper policies could be explored and prove to be more effective, time constraints have limited the implementation of such alternatives.\n",
    "\n",
    "*P.S. +1 at the exam for committing the Lab on the 24th December!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "\n",
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "def print_square():\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            print(MAGIC[i], end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def random_game():\n",
    "    # List of states\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        # First player choices a move\n",
    "        x = choice(list(available))\n",
    "        # State of first player updates\n",
    "        state.x.add(x)\n",
    "        # Append the state, not only the move\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(1_000_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])\n",
    "\n",
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [02:59<00:00, 2779.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training over random games.\n",
    "b = Board()\n",
    "for _ in tqdm(range(500_000)):\n",
    "    b.random_game(epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [02:27<00:00, 2029.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tuning over policy games.\n",
    "for _ in tqdm(range(300_000)):\n",
    "    b.policy_game(agent='greedy', epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1963.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Testing performance.\n",
    "winners = []\n",
    "for _ in tqdm(range(10_000)):\n",
    "    b.policy_game(agent='greedy', epsilon=0)\n",
    "    winners.append(b.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins   : 73.36%\n",
      "Draws  : 11.12%\n",
      "Losses : 15.52%\n"
     ]
    }
   ],
   "source": [
    "print(f'Wins   : {winners.count(-1)/100}%\\nDraws  : {winners.count(0)/100}%\\nLosses : {winners.count(1)/100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
